# Chapter 2: Prepare Phase

## 1. Data sources

For this analysis, I am using three data sources, which provide different perspectives on user trends and relationships with their smart devices:

- **A) Fitbit fitness tracker dataset (Kaggle)**

Available [here](https://www.kaggle.com/datasets/arashnic/fitbit), this dataset, recommended by Bellabeat's COO, was generated by respondents to a distributed survey via Amazon Mechanical Turk between March 12, 2016, and May 12, 2016. Thirty eligible Fitbit users consented to share their personal tracker data. Individual reports can be parsed by export session ID or timestamp.

However, because of this dataset's limitations (small sample size, data collected in 2016, and lack of gender differentiation), I looked for a complementary data source (see below).

- **B) Survey dataset about people's relationships wwith their smart devices (MDPI)**

Available [here](https://www.mdpi.com/2306-5729/9/4/56), this dataset includes survey responses from over 500 individuals, collected between May and July 2020. The larger, more recent sample makes it more representative and relevant. Additionally, the survey asked respondents whether they were male (1) or female (2), which makes it better suited to Bellabeat’s focus on women. Lastly, this dataset adds a psychological dimension to the analysis by exploring user attitudes and interactions with their smart devices, which is nice addition to the Fitbit data.

- **C) Reddit sentiment analyis on "smartwatches" sub-reddits**

To complement the Fitbit and MDPI survey datasets, I collected public sentiment data from Reddit API (accessed via the `praw` library in Python). To align with my business task, I focused on discussions related to trends and opinions about smartwatches. Like the survey dataset, the Reddit dataset helps understand users' feelings and attitudes towards smart devices. However, it captures these sentiments through open discussions in Reddit forums, which might provide more authentic and spontaneous user opinions compared to a structured survey.



## 2. Data organization

### Overall structure for the Bellabeat project

The project is organized as follows:

BELLABEAT: Project main directory.
Contains four main branches (DATA, SCRIPTS, IMAGES, REPORTS):

  - DATA: All the datasets used in this project
    - VAULT: Original, unmodified datasets (Fitbit and Survey)
    - Fitbit: Working directory for Fitbit data
       - Fitbit_Complete_Data: Complete Fitbit data, before cleaning
       - Cleaned_Fitbit: Cleaned Fitbit data, ready for analysis
       - BigQuery_Exports: CSV files exported from BigQuery
    - Survey: Working directory for Survey data
       - Survey_Data: Survey data, before cleaning
       - Cleaned_Survey: Cleaned Survey data, ready for analysis
       - BigQuery_Exports: CSV files exported from BigQuery

  - SCRIPTS: All the scripts used in this project
    - R: R scripts
    - Python: Python scripts
    - SQL: SQL queries (BigQuery) 
    - Shell: Shell scripts (bash) 
     
  - IMAGES: All the images and plots in this project
  - REPORTS: All the reports made in this project


## 3. Preparing the datasets

### A) Fitbit

While the Survey dataset seems fairly straightforward and contains one single Excel file, the original Fitbit dataset unzips into 2 separate folders:

`Fitabase Data 3.12.16 - 4.11.16` 
and 
`Fitabase Data 4.12.16 - 5.12.16`

Many files from the first Fitbit folder (containing 11 csv files) seem to have a matching filename in the second Fitbit folder (containing 18 csv files), only with different time periods. 

Let's write all the file names into two text files with the command line:

```bash
ls > filelist_1.txt
ls > filelist_2.txt
```

I imported both text files into Google Sheets, pasted them side by side in two columns, then applied an Excel formula to check for identical filenames:

```excel
=IF(ISNUMBER(MATCH(A1, B:B, 0)), "Match", "No Match")
```

*Quick reminder on how the `MATCH()` function works in Excel and Google Sheets:*

* `MATCH()` checks if the value in cell A1 exists anywhere in Column B. The 0 at the end means it looks for an exact match.

* If a match is found, `MATCH()` returns the row number where it found the value in Column B. If no match is found, it returns an error (`#N/A`).

* `ISNUMBER()` checks if the return value is a number, and `IF()` evaluates whether `ISNUMBER()` returns `TRUE` or `FALSE`.

I applied conditional formatting in Google Sheets to highlight cells with matching filenames, which confirmed that all the 11 files from the first folder had "twin" files in the second folder. The second folder contained an additional 7 files not found in the first.

#### Combining the "twin" files

Once the filenames were verified, I concatenated the matching twin files using R and stored the combined files in the `Fitbit_Complete_Data` folder, (along with the remaining files that had no matching filenames). 

Sample code:

```r
# Define folder paths for the original Fitbit datasets
folder1 <- here("BELLABEAT", "DATA", "VAULT", "Fitabase Data 3.12.16 - 4.11.16")
folder2 <- here("BELLABEAT", "DATA", "VAULT", "Fitabase Data 4.12.16 - 5.12.16")

# List files in each folder
files1 <- list.files(folder1, pattern = "*.csv", full.names = TRUE)
files2 <- list.files(folder2, pattern = "*.csv", full.names = TRUE)

# Looping through files to look for matching filenames ("twins")
for (file in unique(c(basename(files1), basename(files2)))) {
 
  # If there are twin files in folders 1 and 2, concat files
  if (file %in% basename(files1) & file %in% basename(files2)) {
    data1 <- read_csv(file.path(folder1, file))
    data2 <- read_csv(file.path(folder2, file))
   
    # Combine the data
    combined_data <- rbind(data1, data2)
   
    # Save the combined file in the "Fitbit_Complete_Data" folder
    write_csv(combined_data, here("BELLABEAT", "DATA", "Fitbit", "Fitbit_Complete_Data", paste0("combined_", file)))
   
  } else {
    # Copy non-matching files to the "Fitbit_Complete_Data" folder
    if (file %in% basename(files1)) {
      file.copy(file.path(folder1, file), here("BELLABEAT", "DATA", "Fitbit", "Fitbit_Complete_Data", file))
    } else {
      file.copy(file.path(folder2, file), here("BELLABEAT", "DATA", "Fitbit", "Fitbit_Complete_Data", file))
    }
  }
}

```
    
#### Verifying that the file combination went well

First I checked if all required files were present in the new, unified folder with the following command line:

```bash
cd Fitbit_Complete_Data/
ls > total_files.txt
cat total_files.txt
wc -l total_files.txt
```

This process resulted in a total of 18 Fitbit .csv files (11 combined "twin" files and 7 additional files from the second folder).

Then, regarding the 11 combined files, I made sure that the combined nrows =  nrows1 + nrows2 with an R script.

Sample code:

```R
# Get the list of combined files (only those starting with "combined_")
combined_files <- list.files(combined_dir, pattern = "^combined.*csv$", full.names = TRUE)


# Loop through combined files and check row counts
for (file in combined_files) {
  basefile <- basename(file)
  
  # Remove "combined_" prefix to match files in dir1 and dir2
  basefile_no_combined <- sub("combined_", "", basefile)
  
  # Read files from both directories and the combined directory
  data_combined <- read.csv(file)
  data_dir1 <- read.csv(file.path(dir1, basefile_no_combined))
  data_dir2 <- read.csv(file.path(dir2, basefile_no_combined))
  
  # Get row counts
  count_combined <- nrow(data_combined)
  count_dir1 <- nrow(data_dir1)
  count_dir2 <- nrow(data_dir2)
  
  # Calculate the expected total 
  expected_total <- count_dir1 + count_dir2
  
  # Calculate the difference
  diff <- expected_total - count_combined
  
  if (diff != 0) {
    cat("ERROR in count!")
  }
  
  # Output results
  cat("File:", basefile, "\n")
  cat("Dir 1 Count:", count_dir1, "\n")
  cat("Dir 2 Count:", count_dir2, "\n")
  cat("Expected total: ", expected_total, "\n")
  cat("Combined Count:", count_combined, "\n")
  cat("Difference in count is", diff, "\n")
  cat("----------------------------\n")
}
```


### B) Survey

The Survey dataset consisted of a single Excel file, so there was no need to prepare it further.


### C) Reddit

I retrieved the Reddit dataset by running a Python scraping script. The script uses the `praw` library to connect to the Reddit API and collect posts related to smartwatches. The retrieved data is then analyzed for sentiment and stored in two CSV files: one for the top posts and another for the most recent posts.

Sample code

``` python
import praw
from textblob import TextBlob
import csv


def save_posts_to_csv(posts, filename):
    # Open CSV file in write mode
    with open(filename, mode='w', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)
        writer.writerow(["Title", "Score", "Sentiment", "Polarity", "URL"])

        for post in posts:
            title = post.title
            score = post.score
            url = post.url

            # Perform sentiment analysis
            sentiment = TextBlob(title).sentiment.polarity
            sentiment_label = "Positive" if sentiment > 0 else "Negative" if sentiment < 0 else "Neutral"

            # Write data to CSV file
            writer.writerow([title, score, sentiment_label, sentiment, url])
            
            print(f"Title: {title}")
            print(f"Score: {score}")
            print(f"Sentiment: {sentiment_label} (Polarity: {sentiment:.2f})")
            print(f"URL: {url}\n")

def get_top_posts(keyword):
    subreddit = reddit.subreddit("all")
    # Search for top posts with the keyword "smartwatch"
    top_posts = subreddit.search(keyword, sort="top", limit=10)
    save_posts_to_csv(top_posts, './Reddit_top_posts_smartwatch.csv')  # Save to CSV

def get_new_posts(keyword):
    subreddit = reddit.subreddit("all")
    # Search for the most recent posts with the keyword "smartwatch"
    new_posts = subreddit.search(keyword, sort="new", limit=10)
    save_posts_to_csv(new_posts, './Reddit_new_posts_smartwatch.csv')  # Save to CSV

if __name__ == "__main__":
    get_top_posts("smartwatch")  # Search for top posts about "smartwatch"
    get_new_posts("smartwatch")  # Search for new posts about "smartwatch"

```


## 4. Datasets ready for cleaning

Now, the Fitbit dataset is fully organized, with all the files stored in one unified folder called `DATA/Fitbit/Fitbit_Complete_Data`.
The survey data is stored separately in a folder called `DATA/Survey`.
As for the Reddit data, it is stored in a folder called `DATA/Reddit`. 

### A) `Fitbit_Complete_Data` folder

This folder contains 18 .csv files with data from March 12 to May 12, 2016, organized as follows:

File                     | Description
-------------------------|-----------------------------
`dailyActivity_merged.csv` | Daily summary of activity levels, steps, and calories burned.
`heartrate_seconds_merged.csv` | Second-by-second heart rate data.
`hourlyCalories_merged.csv` | Hourly calorie data.
`hourlyIntensities_merged.csv` | Hourly intensity data.
`hourlySteps_merged.csv` | Hourly step data.
`minuteCaloriesNarrow_merged.csv` | Narrow minute-level calorie data.
`minuteIntensitiesNarrow_merged.csv` | Narrow minute-level intensity data.
`minuteMETsNarrow_merged.csv` | Narrow minute-level MET data.
`minuteSleep_merged.csv` | Minute-level sleep data.
`minuteStepsNarrow_merged.csv` | Narrow minute-level step data.
`weightLogInfo_merged.csv` | Weight log information.
`dailyCalories_merged.csv` | Daily calorie data.
`dailyIntensities_merged.csv` | Daily intensity data.
`dailySteps_merged.csv` | Daily step data.
`minuteCaloriesWide_merged.csv` | Wide minute-level calorie data.
`minuteIntensitiesWide_merged.csv` | Wide minute-level intensity data.
`minuteStepsWide_merged.csv` | Wide minute-level step data.
`sleepDay_merged.csv` | Daily summary of sleep data.

### B) `Survey` folder

This folder contains:
- A `Data Report` PDF file providing the research context, names of the survey authors, etc.
- The original blank survey questionnaire (PDF)
- The actual dataset in Excel format.
  
The Excel file contains data collected from over 500 individuals between May and July 2020. It focuses on user opinions and interactions with their smart devices:

File                     | Description
-------------------------|-----------------------------
`Anonymized_UserRelationshipWithTheirSmartDevice_Dataset.xlsx` | User opinions and interactions with their smart devices.

The Excel file is in long (narrow) format.

### C) `Reddit` folder

This folder contains:
- Two CSV files called `Reddit_top_posts_smartwatch.csv` and `Reddit_new_posts_smartwatch.csv`.

These files contain data collected from a Python scraping script that retrieves the Top 10 most popular and Top 10 most recent posts about "smartwatches" on Reddit:

Files                    | Description
-------------------------|-----------------------------
`Reddit_top_posts_smartwatch.csv` | Most popular posts about smartwatches.
`Reddit_new_posts_smartwatch.csv` | Most recent posts about smartwatches.


## 5. Credibility and limitations

### A) Fitbit dataset

#### Context and authors

The Fitbit dataset was compiled and made publicly available by [Möbius](https://www.kaggle.com/arashnic) on [Kaggle](https://www.kaggle.com/datasets/arashnic/fitbit). The data was collected through Fitbit’s API, and generated by respondents to a distributed survey via Amazon Mechanical Turk between March 12, 2016, and May 12, 2016. Thirty eligible Fitbit users consented to share their personal tracker data. 

#### Limitations

The Fitbit dataset has four key limitations:

- a) Small sample size (30 users): not representative, makes it difficult to draw conclusions that can be applied to a larger population;
- b) Outdated data (collected in 2016): the dataset may not reflect current trends in smart device usage;
- c) Lack of gender differentiation: since Bellabeat focuses on women's health and wellness, the lack of gender-specific data makes it less relevant for this business task.
- d) Potential bias: Fitbit users might be more active than the general population, which could introduce bias in the analysis.


### B) Survey dataset

#### Context and authors

The survey dataset was conducted by [Francesco Lelli](https://francescolelli.info/) (Professor at Tilburg University) and [Heidi Toivonen](https://www.heiditoivonen.com/) (PostDoc at Ghent University) using the Qualtrics platform. Data was collected from over 500 participants between May and July 2020, with no specific pre-selection of respondents. 

#### Strengths

- a) Larger sample size (500+ participants): provides a more robust and representative perspective;
- b) More recent data (collected in 2020): provides a more recent perspective;
- c) Gender differentiation: includes data that distinguishes between men and women (coded as '1' or '2'), which makes it more relevant to Bellabeat’s focus on women;
- d) Ethical approval: received a stamp of ethical approval (IRB EXE 2020-007) for data collection integrity and participant protection.
- e) Global collection: distributed globally, so there was a diverse pool of respondents, which adds to the dataset's generalizability.
- f) Psychological insights: adds an understanding of how users feel about and interact with their smart devices.

#### Limitations

However, the survey respondents might still represent a specific demographic and could limit the generalization of the results.


### C) Reddit dataset

#### Context

The Reddit dataset was collected using the Reddit API, accessed via the `praw` library in Python. The data was collected from various subreddits discussing smartwatches, focusing on the most recent and the most popular posts. This dataset provides a unique perspective on user sentiment, by capturing spontaneous discussions about smartwatches in public forums.

#### Strengths

- a) **Authenticity of user opinions**: Reddit discussions are typically organic and spontaneous, which may offer more genuine insights into user attitudes compared to a structured survey.
- b) **Real-time relevance**: The dataset includes recent posts, so they provide us with the most up-to-date user discussions on smartwatches.
- c) **User engagement**: Posts with high engagement (upvotes, comments) provide insights into the topics that are most popular with the community.
- d) **Diverse subreddits**: Data was collected from multiple subreddits in order to get a broader range of opinions.

#### Limitations

- a) **Selection bias**: Reddit users are not representative of the general population. They may be younger, more tech-savvy, and more opinionated than average.
- b) **Limited demographic data**: Unlike the survey dataset, Reddit posts do not include demographic information, such as gender or location, so that limits the ability to segment the data.
- c) **Potential bias in post visibility**: Posts with higher engagement are more likely to be seen and upvoted, which could skew the analysis towards more popular opinions.
- d) **Content variability**: Reddit posts can vary greatly in tone, length, and detail, which can make sentiment analysis challenging and potentially less precise.



## 6. Data integrity and privacy

The Fitbit and Survey datasets are GDPR-compliant and do not contain any personally identifiable information. They are publicly available and licensed for open use (Fitbit data via Kaggle and the survey data via MDPI's open access). Both datasets have been downloaded and stored locally to preserve privacy and data integrity before moving to the cleaning phase.

As for the Reddit dataset, the data collected via the Reddit API contains only publicly available information (i.e. post titles, scores and URLs). The dataset does not include user identities or any sensitive information, which complies with privacy guidelines. And just like the Fitbit and the Survey datasets, the Reddit data is stored locally to prevent unnecessary exposure and preserve the integrity of the dataset.
